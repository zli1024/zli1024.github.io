[{"content":"实际光学系统的基点位置和焦距的计算 在学习《工程光学》第4版第二章第二节时，想对比下近轴光路光线追迹的常规计算公式和矩阵方法有何区别，于是动手实际算了下。\n由前知，共轴球面系统的近轴区就是实际的理想光学系统，在实际系统的近轴区追迹平行于光轴的光线，就可以计算出实际系统近轴区的基点位置和焦距，通常实际系统就以此作为它的基点和焦距。\n下面以如图2-12所示的三片型照相物镜为例描述具体的计算过程和计算结果。\n1. 近轴光路计算公式（逐面计算） 所用公式如下： $$ \\begin{aligned} i \u0026= \\frac{l-r}{r}u \\\\ i^{\\prime} \u0026= \\frac{n}{n^{\\prime}}i \\\\ u^{\\prime} \u0026= u + i - i^{\\prime} \\\\ l^{\\prime} \u0026= r(1+\\frac{i^{\\prime}}{u^{\\prime}}) \\end{aligned} $$ 第一面： $$ \\begin{aligned} i_2 \u0026= \\frac{n_1}{n_2}i_1 \\\\ \u0026= \\frac{1}{1.6140}\\cdot \\frac{10}{26.67} = 0.2323 \\\\ u_2 \u0026= u_1 + i_1 -i_2 = \\frac{10}{26.67} - 0.2323 = 0.1427 \\\\ l_2 \u0026= r_1(1+\\frac{i_2}{u_2}) = 26.67\\times(1+\\frac{0.2323}{0.1427})=70.0858 \\end{aligned} $$ 第二面： $$ \\begin{aligned} i_3 \u0026= \\frac{l_2 - d_1 - r_2}{r_2}\\times u_2 \\\\ \u0026= \\frac{70.0858 - 5.20 - 189.67}{189.67}\\times 0.1427 = -0.0939 \\\\ i_4 \u0026= \\frac{n_2}{n_3}i_3 = \\frac{1.6140}{1}\\cdot (-0.0939) = -0.1516 \\\\ u_3 \u0026= u_2 + i_3 - i_4 = 0.1427 + (-0.0939) + 0.1516 = 0.2004 \\\\ l_3 \u0026= r_2(1+\\frac{i_4}{u_3}) = 189.67\\times (1+\\frac{-0.1516}{0.2004}) = 46.1871 \\end{aligned} $$ 第三面： $$ \\begin{aligned} i_5 \u0026= \\frac{l_3 - d_2 - r_3}{r_3}\\times u_3 \\\\ \u0026= \\frac{46.1871 - 7.95 + 49.66}{-49.66}\\times 0.2004 = -0.3547 \\\\ i_6 \u0026= \\frac{n_3}{n_4}i_5 = \\frac{1}{1.6475}\\cdot (-0.3547) = -0.2153 \\\\ u_4 \u0026= u_3 + i_5 - i_6 = 0.2004 - 0.3547 + 0.2153 = 0.0610 \\\\ l_4 \u0026= r_3(1+\\frac{i_6}{u_4}) = -49.66\\times (1+\\frac{-0.2153}{0.0610}) = 125.6154 \\end{aligned} $$ 第四面： $$ \\begin{aligned} i_7 \u0026= \\frac{l_4 - d_3 - r_4}{r_4}\\times u_4 \\\\ \u0026= \\frac{125.6154 - 1.6 - 25.47}{25.47}\\times 0.0610 = 0.2360 \\\\ i_8 \u0026= \\frac{n_4}{n_5}i_7 = \\frac{1.6475}{1}\\cdot 0.2360 = 0.3888 \\\\ u_5 \u0026= u_4 + i_7 - i_8 = 0.0610 + 0.2360 - 0.3888 = -0.0918 \\\\ l_5 \u0026= r_4(1+\\frac{i_8}{u_5}) = 25.47\\times (1+\\frac{0.3888}{-0.0918}) = -82.4029 \\end{aligned} $$ 第五面： $$ \\begin{aligned} i_9 \u0026= \\frac{l_5 - d_4 - r_5}{r_5}\\times u_5 \\\\ \u0026= \\frac{-82.4029 - 6.7 - 72.11}{72.11}\\times (-0.0918) = 0.2052 \\\\ i_{10} \u0026= \\frac{n_5}{n_6}i_9 = \\frac{1}{1.6140}\\cdot 0.2052 = 0.1271 \\\\ u_6 \u0026= u_5 + i_9 - i_{10} = -0.0918 + 0.2052 - 0.1271 = -0.0137 \\\\ l_6 \u0026= r_5(1+\\frac{i_{10}}{u_6}) = 72.11\\times (1+\\frac{0.1271}{-0.0137}) = -596.8813 \\end{aligned} $$ 第六面： $$ \\begin{aligned} i_{11} \u0026= \\frac{l_6 - d_5 - r_6}{r_6}\\times u_6 \\\\ \u0026= \\frac{-596.8813 - 2.8 + 35.00}{-35.00}\\times (-0.0137) = -0.2210 \\\\ i_{12} \u0026= \\frac{n_6}{n_7}i_{11} = \\frac{1.6140}{1}\\cdot (-0.2210) = -0.3567 \\\\ u_7 \u0026= u_6 + i_{11} - i_{12} = -0.0137 - 0.2210 + 0.3567 = 0.1220 \\\\ l_7 \u0026= r_6(1+\\frac{i_{12}}{u_7}) = -35.00\\times (1+\\frac{-0.3567}{0.1220}) = 67.3320 \\end{aligned} $$ 所以得到的结果：$l^{\\prime} = 67.3320mm$, $u^{\\prime}=0.1220$\n书中的结果为：$l^{\\prime} = 67.4907mm$, $u^{\\prime}=0.121869$\n误差应该是由matlab运算时精度的不同导致的。\n2. 光线追迹矩阵方法 这种方法是根据空间中每一点在子午面上都能由它的高度$y$ 和角度$\\alpha$ 唯一决定（近轴区）。\n详见ECE 6510 Lecture 4\n初始值： $$ \\begin{bmatrix} y_0 \\\\ \\alpha_0 \\end{bmatrix} = \\begin{bmatrix} 10 \\\\ 0 \\end{bmatrix} $$ 第一面： $$ R_1 = \\begin{bmatrix} 1 \u0026 0 \\\\ \\frac{1}{r_1}(\\frac{n_1}{n_2} - 1) \u0026 \\frac{n_1}{n_2} \\end{bmatrix} = \\begin{bmatrix} 1 \u0026 0 \\\\ \\frac{1}{26.67}(\\frac{1}{1.6140} - 1) \u0026 \\frac{1}{1.6140} \\end{bmatrix} = \\begin{bmatrix} 1 \u0026 0 \\\\ -0.0143 \u0026 0.6196 \\end{bmatrix} $$$$ T_1 = \\begin{bmatrix} 1 \u0026 d_1 \\\\ 0 \u0026 1 \\end{bmatrix} = \\begin{bmatrix} 1 \u0026 5.20 \\\\ 0 \u0026 1 \\end{bmatrix} $$第二面： $$ R_2 = \\begin{bmatrix} 1 \u0026 0 \\\\ \\frac{1}{r_2}(\\frac{n_2}{n_3} - 1) \u0026 \\frac{n_2}{n_3} \\end{bmatrix} = \\begin{bmatrix} 1 \u0026 0 \\\\ \\frac{1}{189.67}(\\frac{1.6140}{1} - 1) \u0026 1.6140 \\end{bmatrix} = \\begin{bmatrix} 1 \u0026 0 \\\\ 0.0032 \u0026 1.6140 \\end{bmatrix} $$$$ T_2 = \\begin{bmatrix} 1 \u0026 d_2 \\\\ 0 \u0026 1 \\end{bmatrix} = \\begin{bmatrix} 1 \u0026 7.95 \\\\ 0 \u0026 1 \\end{bmatrix} $$第三面： $$ R_3 = \\begin{bmatrix} 1 \u0026 0 \\\\ \\frac{1}{r_3}(\\frac{n_3}{n_4} - 1) \u0026 \\frac{n_3}{n_4} \\end{bmatrix} = \\begin{bmatrix} 1 \u0026 0 \\\\ \\frac{1}{-49.66}(\\frac{1}{1.6475} - 1) \u0026 \\frac{1}{1.6475} \\end{bmatrix} = \\begin{bmatrix} 1 \u0026 0 \\\\ 0.0079 \u0026 0.6070 \\end{bmatrix} $$$$ T_3 = \\begin{bmatrix} 1 \u0026 d_3 \\\\ 0 \u0026 1 \\end{bmatrix} = \\begin{bmatrix} 1 \u0026 1.6 \\\\ 0 \u0026 1 \\end{bmatrix} $$第四面： $$ R_4 = \\begin{bmatrix} 1 \u0026 0 \\\\ \\frac{1}{r_4}(\\frac{n_4}{n_5} - 1) \u0026 \\frac{n_4}{n_5} \\end{bmatrix} = \\begin{bmatrix} 1 \u0026 0 \\\\ \\frac{1}{25.47}(\\frac{1.6475}{1} - 1) \u0026 1.6475 \\end{bmatrix} = \\begin{bmatrix} 1 \u0026 0 \\\\ 0.0254 \u0026 1.6475 \\end{bmatrix} $$$$ T_4 = \\begin{bmatrix} 1 \u0026 d_4 \\\\ 0 \u0026 1 \\end{bmatrix} = \\begin{bmatrix} 1 \u0026 6.7 \\\\ 0 \u0026 1 \\end{bmatrix} $$第五面： $$ R_5 = \\begin{bmatrix} 1 \u0026 0 \\\\ \\frac{1}{r_5}(\\frac{n_5}{n_6} - 1) \u0026 \\frac{n_5}{n_6} \\end{bmatrix} = \\begin{bmatrix} 1 \u0026 0 \\\\ \\frac{1}{72.11}(\\frac{1}{1.6140} - 1) \u0026 \\frac{1}{1.6140} \\end{bmatrix} = \\begin{bmatrix} 1 \u0026 0 \\\\ -0.0053 \u0026 0.6196 \\end{bmatrix} $$$$ T_5 = \\begin{bmatrix} 1 \u0026 d_5 \\\\ 0 \u0026 1 \\end{bmatrix} = \\begin{bmatrix} 1 \u0026 2.8 \\\\ 0 \u0026 1 \\end{bmatrix} $$第六面： $$ R_6 = \\begin{bmatrix} 1 \u0026 0 \\\\ \\frac{1}{r_6}(\\frac{n_6}{n_7} - 1) \u0026 \\frac{n_6}{n_7} \\end{bmatrix} = \\begin{bmatrix} 1 \u0026 0 \\\\ \\frac{1}{-35.00}(\\frac{1.6140}{1} - 1) \u0026 1.6140 \\end{bmatrix} = \\begin{bmatrix} 1 \u0026 0 \\\\ -0.0175 \u0026 1.6140 \\end{bmatrix} $$所以，系统矩阵为： $$ M = R_6 T_5 R_5 T_4 R_4 T_3 R_3 T_2 R_2 T_1 R_1 = \\begin{bmatrix} 0.8200 \u0026 24.4575 \\\\ -0.0123 \u0026 0.8531 \\end{bmatrix} $$ Check det(M) = 1.0001 符合要求\n所以最后的高 $y_f$ 和角度 $\\alpha_f$ 为 $$ \\begin{bmatrix} y_f \\\\ \\alpha_f \\end{bmatrix} = M \\begin{bmatrix} y_0 \\\\ \\alpha_0 \\end{bmatrix} = \\begin{bmatrix} 0.8200 \u0026 24.4575 \\\\ -0.0123 \u0026 0.8531 \\end{bmatrix} \\begin{bmatrix} 10 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 8.2002 \\\\ -0.1229 \\end{bmatrix} $$ 所以$u^{\\prime}=0.1229$ ，与书中结果误差较大，原因可能是在算translation matrix时将$L$ 近似为 $d$ 。\n结论：矩阵方法简单易算，但误差较大；常规公式计算繁杂但精确\n","permalink":"https://zli1024.github.io/posts/ray-tracing/","summary":"\u003ch2 id=\"实际光学系统的基点位置和焦距的计算\"\u003e实际光学系统的基点位置和焦距的计算\u003c/h2\u003e\n\u003cp\u003e\u003cem\u003e在学习《工程光学》第4版第二章第二节时，想对比下近轴光路光线追迹的常规计算公式和矩阵方法有何区别，于是动手实际算了下。\u003c/em\u003e\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e由前知，共轴球面系统的近轴区就是实际的理想光学系统，在实际系统的近轴区追迹平行于光轴的光线，就可以计算出实际系统近轴区的基点位置和焦距，通常实际系统就以此作为它的基点和焦距。\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003e下面以如图2-12所示的三片型照相物镜为例描述具体的计算过程和计算结果。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://zli1024-pictures.oss-cn-shenzhen.aliyuncs.com/imgs/202506131417544.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"1-近轴光路计算公式逐面计算\"\u003e1. 近轴光路计算公式（逐面计算）\u003c/h3\u003e\n\u003cp\u003e所用公式如下：\n\u003c/p\u003e\n$$\n\\begin{aligned}\ni \u0026= \\frac{l-r}{r}u \\\\\ni^{\\prime} \u0026= \\frac{n}{n^{\\prime}}i \\\\\nu^{\\prime} \u0026= u + i - i^{\\prime} \\\\\nl^{\\prime} \u0026= r(1+\\frac{i^{\\prime}}{u^{\\prime}})\n\\end{aligned}\n$$\u003cp\u003e\n第一面：\n\u003c/p\u003e\n$$\n\\begin{aligned}\ni_2 \u0026= \\frac{n_1}{n_2}i_1 \\\\\n    \u0026= \\frac{1}{1.6140}\\cdot \\frac{10}{26.67} = 0.2323 \\\\\nu_2 \u0026= u_1 + i_1 -i_2 = \\frac{10}{26.67} - 0.2323 = 0.1427 \\\\\nl_2 \u0026= r_1(1+\\frac{i_2}{u_2}) = 26.67\\times(1+\\frac{0.2323}{0.1427})=70.0858\n\\end{aligned}\n$$\u003cp\u003e\n第二面：\n\u003c/p\u003e\n$$\n\\begin{aligned}\ni_3 \u0026= \\frac{l_2 - d_1 - r_2}{r_2}\\times u_2 \\\\\n    \u0026= \\frac{70.0858 - 5.20 - 189.67}{189.67}\\times 0.1427 = -0.0939 \\\\\ni_4 \u0026= \\frac{n_2}{n_3}i_3 = \\frac{1.6140}{1}\\cdot (-0.0939) = -0.1516 \\\\\nu_3 \u0026= u_2 + i_3 - i_4 = 0.1427 + (-0.0939) + 0.1516 = 0.2004 \\\\\nl_3 \u0026= r_2(1+\\frac{i_4}{u_3}) = 189.67\\times (1+\\frac{-0.1516}{0.2004}) = 46.1871\n\\end{aligned}\n$$\u003cp\u003e\n第三面：\n\u003c/p\u003e","title":"近轴光路的光线追迹"},{"content":"Homework 2 CVX-OPT 10-725 1 Subgradients and Proximal Operators (i)\nLet $g_1, g_2 \\in \\partial f(x)$ , therefore we have: $$ \\begin{aligned} f(y) \u0026\\ge f(x) + g_1^\\top (y-x) \\\\ f(y) \u0026\\ge f(x) + g_2^\\top (y-x) \\\\ tf(y) \u0026\\ge tf(x) + tg_1^\\top (y-x) \\qquad \\qquad \\qquad(1)\\\\ (1-t)f(y) \u0026\\ge (1-t)f(x) + (1-t)g_2^\\top (y-x) \\quad (2)\\\\ \\end{aligned} $$ (1)+(2): $$ f(y) \\ge f(x) + [tg_1+(1-t)g_2]^\\top(y-x) $$ Thus, $g=tg_1+(1-t)g_2 \\in \\partial f$ and the subdifferential $\\partial f(x)$ is a convex set.\n(ii)\nRecall that:\nThe normal cone of a set $C$ at a boundary point $x_0$ is the set of all vectors $y$ such that $y^\\top (x-x_0) \\le 0$ for all $x \\in C$ (i.e., the set of vectors that define a supporting hyperplane to $C$ at $x_0$)\nProve: $$ \\partial f(x) \\in N_C(x), \\quad where \\ C=\\{ y: f(y) \\le f(x) \\} $$ Let $y_0$ be the boundary point, thus $f(y_0) = f(x_0), \\ y_0 = x_0 $ and $C=\\{y: f(y)\\le f(x_0) \\}$,\nWe need to prove: $$ \\partial f(x_0)^\\top (y - y_0) \\le 0 $$ for all $y \\in C$.\nWe know that $\\partial f(x_0) = \\{g\\in \\mathbb{R}^n: f(y)\\ge f(x_0)+g^\\top(y-x_0) \\}$, thus: $$ \\begin{aligned} f(y) - f(x_0) \u0026\\ge g^\\top(y-x_0) \\\\ f(y) - f(y_0) \u0026\\ge g^\\top(y-y_0) \\\\ \\end{aligned} $$ And $f(y) - f(y_0) \\le 0$ , so $0 \\ge f(y) - f(y_0) \\ge g^\\top(y-y_0)$ . Thus, $\\partial f(x_0)^\\top (y - y_0) \\le 0$\n(iii)\nLet $z = \\frac{y}{||y||_q}, ||z||_q = 1$. $$ \\begin{aligned} z^\\top x = \\frac{y^\\top x}{||y||_q} \u0026\\le ||x||_p \\\\ y^\\top x \u0026\\le ||x||_p ||y||_q \\end{aligned} $$ (iv)\nWe need to prove: $$ ||y||_p \\ge ||x||_p + g^\\top (y-x), \\quad where \\ g \\in \\partial f(x) $$ Let $z^{*} = argmax_{||z||_q\\le 1} z^\\top x$, therefore, $z^{*\\top} x = ||x||_p$\nThus: $$ ||x||_p + z^{*\\top}(y-x) = ||x||_p + z^{*\\top}y - z^{*\\top}x = z^{*\\top}y \\le ||y||_p $$ Therefore, $z^{*}\\in \\partial f(x)$\n(i) $$ \\begin{aligned} \u0026 \\textbf{prox}_{h,t}(x) = \\mathop{\\arg\\min}_{z} \\ \\frac{1}{2}||z-x||_2^2 + t(\\frac{1}{2}z^\\top Az+b^\\top z+c) \\\\ \\therefore \u0026 \\ \\ (z-x) + t(Az+b) = 0 \\\\ \\therefore \u0026 \\ \\ z^{+} = \\textbf{prox}_{h,t}(x) = (I+tA)^{-1}(x-tb) \\end{aligned} $$ Note that $A\\in \\mathbb{S}_{+}^n$ , and therefore $\\textbf{det}(I+tA) \u003e 0$ $(I+tA)$ is invertible.\n(ii)\nLambert W function: the inverse of the fuction $f(x)=xe^x$; That is, the W function satisfies: $W(x)e^{W(x)} = x$\nThe graph of y = W(x) for real x \u0026lt; 6 and y \u0026gt; −4. The upper branch (blue) with y ≥ −1 is the graph of the function $W_0$ (principal branch), the lower branch (magenta) with y ≤ −1 is the graph of the function $W_{-1}$. The minimum value of x is at {−1/e, −1}\n$$ \\begin{aligned} \u0026\\textbf{prox}_{h,t}(x) = \\mathop{\\arg\\min}_{z}\\frac{1}{2}||z-x||_2^2 + t\\sum_{i=1}^{n}z_i \\log z_i \\\\ \u0026z_i - x_i + t(\\log z_i + 1) = 0 \\\\ \u0026z_i + t\\log z_i = x_i - t \\\\ \u0026\\exp(\\frac{z_i+t\\log z_i}{t}) = \\exp(\\frac{x_i - t}{t}) \\\\ \u0026\\exp(\\frac{z_i}{t})\\exp(\\log z_i) = \\exp(\\frac{x_i - t}{t}) \\\\ \u0026z_i\\exp(\\frac{z_i}{t}) = \\exp(\\frac{x_i - t}{t}) \\\\ \u0026\\frac{z_i}{t}\\exp(\\frac{z_i}{t}) = \\frac{1}{t}\\exp(\\frac{x_i - t}{t}) \\\\ \u0026\\frac{z_i}{t} = W(\\frac{z_i}{t}\\exp(\\frac{z_i}{t})) = W(\\frac{1}{t}\\exp(\\frac{x_i - t}{t})) \\\\ \u0026\\therefore z_i = t W(\\frac{1}{t}\\exp(\\frac{x_i - t}{t})) \\end{aligned} $$(iii)\nTake gradient $\\Longrightarrow 0$ $$ \\begin{aligned} \u0026 (z-x) + t\\frac{z}{||z||_{2}} = 0 \\\\ \u0026 (1+\\frac{t}{||z||_2})z = x \\qquad (1)\\\\ \u0026 ||(1+\\frac{t}{||z||_2})z||_2 = ||x||_2 \\\\ \u0026 (1+\\frac{t}{||z||_2})||z||_2 = ||x||_2 \\\\ \u0026 \\Longrightarrow ||z||_2 = ||x||_2 - t \\qquad (2) \\\\ \u0026 (2) \\rightarrow (1): z = (1-\\frac{t}{||x||_2})x \\end{aligned} $$ (iv)\nTake $i$-th variable, the objective function is: $$ f(x_i) = \\frac{1}{2}(z_i - x_i)^2 + t \\cdot \\textbf{1}_{z_i\\ne 0} $$$$ \\begin{aligned} ① \\ z_i = 0:\u0026 \\\\ \u0026 f(x_i) = \\frac{1}{2}x_i^2 \\\\ ② \\ z_i \\ne 0:\u0026 \\\\ \u0026 \\mathop{\\text{min}}_{z_i} f(x_i) = \\frac{1}{2}(x_i - x_i)^2 + t = t \\\\ \\\\ \\Longrightarrow z_i \u0026= \\begin{cases} x_i, \\quad |x_i| \u003e \\sqrt{2t} \\\\ 0, \\quad |x_i| \\le \\sqrt{2t} \\end{cases} \\end{aligned} $$ (Bonus) \u0026ndash; 待解决\nProximal Operator for the sorted $l_1$ norm\n贴上Chatgpt的回答：\n2 Properties of Proximal Mappings and Subgradients $$ \\begin{aligned} \\text{Prove:}\\qquad\u0026 \\\\ \u0026\\text{If} \\quad v \\in \\text{conv}\\bigg( \\bigcup_{i\\in I(x)} \\partial f_i(x) \\bigg), \\ \\text{then} \\ \\ v \\in \\partial f(x) \\end{aligned} $$ where $I(x):= \\{i: f_i(x)=f(x) \\}$\nProof:\nLet $v = \\sum_i \\lambda_i g_i$ , where $i\\in I(x)$, $g_i \\in \\partial f_i(x)$, $\\lambda_i \\ge 0$, $\\sum_i \\lambda_i = 1$. For a random point $x$: $$ \\begin{aligned} f_i(y) \u0026\\ge f_i(x) + g_i^{T}(y-x) \\\\ \\lambda_i f_i(y) \u0026\\ge \\lambda_i f_i(x) + \\lambda_i g_i^{T}(y-x) \\\\ \\sum_i \\lambda_i f_i(y) \u0026\\ge \\sum_i \\lambda_i f_i(x) + \\sum_i \\lambda_i g_i^{T}(y-x) \\\\ f(y)\\sum_i \\lambda_i \\ge \\sum_i \\lambda_i f_i(y) \u0026\\ge \\sum_i \\lambda_i f_i(x) + \\sum_i \\lambda_i g_i^{T}(y-x) = f(x)\\sum_i \\lambda_i + v^{T}(y-x) \\\\ \\therefore f(y) \u0026\\ge f(x) + v^{T}(y-x) \\\\ \\therefore v \u0026\\in \\partial f(x) \\end{aligned} $$ Proof: $$ prox_t (x) = \\mathop{\\arg\\min}_u \\frac{1}{2t}||u-x||_2^2 + h(u) = \\mathop{\\arg\\min}_u f(u) $$ According to the subgradient optimality condition: $$ \\begin{aligned} \\partial f(u) \\in 0\u0026 \\\\ \\partial (\\frac{1}{2t}||u-x||_2^2 + h(u)) \\in 0\u0026 \\\\ \\frac{1}{t}(u-x) + \\partial h(u) \\in 0\u0026 \\\\ \\partial h(u) \\in \\frac{1}{t}(x-u)\u0026 \\\\ \\Longrightarrow h(y) \\ge h(u) + g^{T}(y-u) \\\\ \\Longrightarrow h(y) \\ge h(u) + \\frac{1}{t}(x-u)^{T}(y-u), \\ \\forall y \\end{aligned} $$ $$ \\begin{aligned} \\text{prox}_f(x) \u0026= \\mathop{\\arg\\min}_u \\frac{1}{2} ||u-x||_2^2 + f(u) \\\\ \u0026= \\mathop{\\arg\\min}_u \\frac{1}{2} ||u-x||_2^2 + g(Au+b) \\\\ \\end{aligned} $$ $$ \\begin{aligned} \\min_u \\frac{1}{2}||u-x||_2^2 \u0026+ g(Au+b) \\\\ \u0026\\Downarrow \\\\ \\min_u \\frac{1}{2}||u-x||_2^2 \u0026+ g(z) \\\\ s.t. \\ \\ z = Au \u0026+ b \\\\ \\end{aligned} $$ $$ \\mathcal{L}(u,z;\\lambda) = \\frac{1}{2}||u-x||_2^2 + g(z) + \\lambda^{T}(z-Au-b) $$ $$ \\begin{cases} \\partial_u \\mathcal{L} = (u-x) - A^{T}\\lambda = 0 \\qquad (1)\\\\ \\partial_z \\mathcal{L} = \\partial g(z) + \\lambda = 0 \\ \\qquad \\qquad (2)\\\\ \\partial_{\\lambda} \\mathcal{L} = z - Au - b = 0 \\ \\ \\quad \\qquad (3) \\end{cases} $$ Plug $(1)$ into $(2)$: $$ \\begin{aligned} z \u0026= Au + b \\\\ \u0026= A(x+A^T\\lambda) + b \\\\ \u0026= Ax + AA^T\\lambda + b \\\\ \u0026= Ax + a\\lambda + b \\qquad (4) \\end{aligned} $$ Plug $(4)$ into $(1)$: $$ \\begin{aligned} u \u0026= x + A^T\\lambda \\\\ \u0026= x + \\frac{1}{a}A^T(z - Ax - b) \\end{aligned} $$ Why? $z = \\text{prox}_{ag}(Ax+b)$\nLHS: $$ \\begin{aligned} u^{+} = \\text{prox}_{f+g} (x) \u0026= \\mathop{\\arg\\min}_u \\ \\frac{1}{2}||u-x||_2^2 + f(u) + g(u) \\\\ \u0026\\Downarrow \\\\ \\partial (\\frac{1}{2}\u0026||u-x||_2^2 + f(u) + g(u)) = 0 \\\\ u \u0026- x + \\partial f(u) + \\partial g(u) = 0 \\end{aligned} $$ RHS: Let $y \\in \\text{dom}(g)$ , $$ \\begin{aligned} y^{+} \u0026= \\text{prox}_g(x) = \\mathop{\\arg\\min}_y \\frac{1}{2}||y-x||_2^2 + g(y) \\\\ \u0026\\Downarrow \\\\ y \u0026- x + \\partial g(y) \\in 0, \\quad \\partial g(y) \\in x - y \\end{aligned} $$ $$ \\begin{aligned} u^{+} \u0026= \\text{prox}_f(\\text{prox}_g(x)) = \\text{prox}_f(y) = \\mathop{\\arg\\min}_u \\ \\frac{1}{2} ||u - y||_2^2 + f(u) \\\\ \u0026\\Downarrow \\\\ \u0026 u - y + \\partial f(u) \\in 0 \\\\ \\end{aligned} $$ Because $\\forall y \\in \\text{dom}(g)$, $\\partial g (\\text{prox}_f (y)) \\supseteq \\partial g(y)$ , $$ \\begin{aligned} \u0026\\therefore \\partial g(\\text{prox}_f (y)) = \\partial g(u) \\in x - y \\\\ \\\\ \u0026\\begin{cases} y - x + \\partial g(u) \\in 0 \\\\ u - y + \\partial f(u) \\in 0 \\end{cases} \\\\ \u0026\\therefore u - x+ \\partial g(u) + \\partial f(u) \\in 0 \\end{aligned} $$3 Convergence Rate for Proximal Gradient Descent $$ \\begin{aligned} g(x^+) \u0026\\le g(x) + \\nabla g(x)^T (-\\frac{1}{L}\\nabla g(x)) + \\frac{L}{2}||-\\frac{1}{L}\\nabla g(x)||_2^2 \\\\ \u0026=g(x) - \\frac{1}{L}||\\nabla g(x)||_2^2 + \\frac{1}{2L}||\\nabla g(x)||_2^2 \\\\ \u0026=g(x) - \\frac{1}{2L}||\\nabla g(x)||_2^2 \\\\ \\therefore \\ \\ \u0026 g(x^+) - g(x) \\le - \\frac{1}{2L}||\\nabla g(x)||_2^2 \\end{aligned} $$ $$ \\begin{aligned} g(z) \u0026\\ge g(x) + \\nabla g(x)^T(z-x) \\\\ g(x) - g(z) \u0026\\le \\nabla g(x)^T (x - z) \\end{aligned} $$ Plug into the inequality of $(i)$ : $$ g(x^+) - g(z) \\le \\nabla g(x)^T(x - z) - \\frac{1}{2L}||\\nabla g(x)||_2^2 $$ From $(ii)$: $$ g(x^+) - g(x^*) \\le \\nabla g(x)^T(x-x^*) - \\frac{1}{2L}||\\nabla g(x)||_2^2 $$ $$ g(x^+) - g(x^*) \\le \\frac{L}{2}\\bigg(\\frac{2}{L} \\nabla g(x)^T(x-x^*) - \\frac{1}{L^2}||\\nabla g(x)||_2^2 \\bigg) $$ $$ g(x^+) - g(x^*) \\le \\frac{L}{2}\\bigg(2t \\nabla g(x)^T(x-x^*) - t^2 ||\\nabla g(x)||_2^2 \\bigg) $$ $$ g(x^+) - g(x^*) \\le \\frac{L}{2}\\bigg( ||x||^2 -2x^Tx^* - ||x||^2 - t^2||\\nabla g(x)||^2 + 2t \\nabla g(x)^Tx + 2x^Tx^* -2t\\nabla g(x)^T x^* \\bigg) $$ $$ g(x^+) - g(x^*) \\le \\frac{L}{2} \\bigg( ||x||^2 - 2x^Tx^* - ||x^+||^2 + 2x^{+T}x^* \\bigg) $$ $$ g(x^+) - g(x^*) \\le \\frac{L}{2} \\bigg( ||x-x^*||^2 - ||x^+ - x^*||^2 \\bigg) $$ $$ \\begin{aligned} g(x^{(1)}) - g(x^*) \u0026\\le \\frac{L}{2} \\bigg( ||x^{(0)}-x^*||^2 - ||x^{(1)} - x^*||^2 \\bigg) \\\\ g(x^{(2)}) - g(x^*) \u0026\\le \\frac{L}{2} \\bigg( ||x^{(1)}-x^*||^2 - ||x^{(2)} - x^*||^2 \\bigg) \\\\ \u0026\\vdots \\\\ g(x^{(k)}) - g(x^*) \u0026\\le \\frac{L}{2} \\bigg( ||x^{(k-1)}-x^*||^2 - ||x^{(k)} - x^*||^2 \\bigg) \\end{aligned} $$ Sum both sides: $$ \\sum_{i=1}^k g(x^{(i)}) - k g(x^*) \\le \\frac{L}{2}\\bigg( ||x^{(0)}-x^*||^2 - ||x^{(k)} - x^*||^2 \\bigg) $$ LHS: $$ \\sum_{i=1}^k g(x^{(i)}) - k g(x^*) \\ge k g(x^{(k)}) - kg(x^*) $$ RHS: $$ \\frac{L}{2}\\bigg( ||x^{(0)}-x^*||^2 - ||x^{(k)} - x^*||^2 \\bigg) \\le \\frac{L}{2} ||x^{(0)}-x^*||^2 $$ Therefore: $$ g(x^{(k)}) - g(x^*) \\le \\frac{L}{2k} ||x^{(0)} - x^*||^2 $$ Sublinear convergence!\n$$ g(x^+) \\le g(x) + \\nabla g(x)^T (x^+ - x) + \\frac{L}{2}||x^+ - x||_2^2 $$ $$ g(x^+) - g(x) \\le \\nabla g(x)^T (x^+ - x) + \\frac{L}{2}||x^+ - x||_2^2 $$ Plug $G(x)$ into the above inequality: $$ g(x^+) - g(x) \\le -\\frac{1}{L}\\nabla g(x)^T G(x) + \\frac{1}{2L}||G(x)||^2 $$ $$ g(z) \\ge g(x) + \\nabla g(x)^T (z - x) $$ $$ g(x) - g(z) \\le \\nabla g(x)^T (x - z) $$ Plus to $(i)$: $$ \\begin{aligned} g(x^+) - g(z) \u0026\\le \\nabla g(x)^T (x - z) -\\frac{1}{L}\\nabla g(x)^T G(x) + \\frac{1}{2L}||G(x)||^2 \\\\ \\Downarrow \\\\ g(x^+) - g(z) \u0026\\le \\nabla g(x)^T (x^+ - z) + \\frac{1}{2t}||x - x^+||^2 \\qquad (1) \\end{aligned} $$ According to Q2 part (b): $$ \\begin{aligned} \u0026\\because x^+ = \\text{prox}_{th} (x - t\\nabla g(x)) \\\\ \u0026\\therefore h(z) \\ge h(x^+) + \\frac{1}{t}(x-t\\nabla g(x) - x^+)^T(z - x^+), \\ \\ \\forall z \\in \\mathbb{R}^n \\\\ \u0026\\Downarrow \\\\ \u0026\\therefore h(x^+) - h(z) \\le -\\frac{1}{t}(x-x^+)^T (z - x^+) + \\nabla g(x)^T (z-x^+) \\qquad (2) \\end{aligned} $$ (1)+(2): $$ f(x^+) - f(z) \\le G(x)^T (x^+ - z) + \\frac{1}{2L}||G(x)||^2 $$ $$ \\begin{aligned} \u0026G(x)^T (x^+ - z) + \\frac{1}{2L}||G(x)||^2 \\\\ =\u0026 (x-x^+)^T(\\frac{1}{2t}x + \\frac{1}{2t}x^+ - \\frac{1}{t}z) \\\\ =\u0026 (x-x^+)^T(\\frac{1}{t}x - \\frac{1}{2t}x + \\frac{1}{2t}x^+ - \\frac{1}{t}z) \\\\ =\u0026 (x-x^+)^T(\\frac{1}{t}(x - z) - \\frac{1}{2t}(x-x^+)) \\\\ =\u0026 G(x)^T (x-z) - \\frac{1}{2L}||G(x)||^2 \\end{aligned} $$ Therefore: $$ f(x^+) - f(z) \\le G(x)^T (x-z) - \\frac{1}{2L}||G(x)||^2 $$ According to (ii): $$ f(x^+) - f(x^*) \\le G(x)^T(x-x^*) - \\frac{1}{2L}||G(x)||^2 $$ $$ f(x^+) - f(x^*) \\le \\frac{L}{2}\\bigg( ||x||^2 - 2x^Tx^* + 2x^{+T}x^* - ||x^+||^2 \\bigg) $$ $$ f(x^+) - f(x^*) \\le \\frac{L}{2}\\bigg( ||x||^2 - 2x^Tx^* + \\textcolor{red}{||x^*||^2} - \\textcolor{red}{||x^*||^2} + 2x^{+T}x^* - ||x^+||^2 \\bigg) $$ $$ f(x^+) - f(x^*) \\le \\frac{L}{2}\\bigg( ||x-x^*||^2 - ||x^+ - x^*||^2 \\bigg) $$ Now, prove: $$ f(x^{(k)}) - f(x^*) \\le \\frac{L}{2k} ||x^{(0)} - x^*||^2 $$ $$ \\begin{aligned} f(x^{(1)}) - f(x^*) \u0026\\le \\frac{L}{2} \\bigg( ||x^{(0)}-x^*||^2 - ||x^{(1)} - x^*||^2 \\bigg) \\\\ f(x^{(2)}) - f(x^*) \u0026\\le \\frac{L}{2} \\bigg( ||x^{(1)}-x^*||^2 - ||x^{(2)} - x^*||^2 \\bigg) \\\\ \u0026\\vdots \\\\ f(x^{(k)}) - f(x^*) \u0026\\le \\frac{L}{2} \\bigg( ||x^{(k-1)}-x^*||^2 - ||x^{(k)} - x^*||^2 \\bigg) \\end{aligned} $$ Sum both sides: $$ \\sum_{i=1}^k f(x^{(i)}) - k f(x^*) \\le \\frac{L}{2}\\bigg( ||x^{(0)}-x^*||^2 - ||x^{(k)} - x^*||^2 \\bigg) $$ LHS: $$ \\sum_{i=1}^k f(x^{(i)}) - k f(x^*) \\ge k f(x^{(k)}) - kf(x^*) $$ RHS: $$ \\frac{L}{2}\\bigg( ||x^{(0)}-x^*||^2 - ||x^{(k)} - x^*||^2 \\bigg) \\le \\frac{L}{2} ||x^{(0)}-x^*||^2 $$ Therefore: $$ f(x^{(k)}) - f(x^*) \\le \\frac{L}{2k} ||x^{(0)} - x^*||^2 $$\nProof: According to the note, $$ \\begin{aligned} x^+ \u0026= \\text{prox}_{t,h}(x-t\\nabla g(x)) = \\mathop{\\arg\\min}_y \\frac{1}{2t}||y-(x-t\\nabla g(x))||_2^2 + h(y) \\\\ \u0026= \\mathop{\\arg\\min}_y \\frac{1}{2t}\\bigg( \\textcolor{green}{||y||^2} - \\textcolor{green}{2y^Tx} + \\textcolor{red}{2t\\nabla g(x)^T y} + \\textcolor{green}{||x||^2} + t^2||\\nabla g(x)||^2 - \\textcolor{red}{2t\\nabla g(x)^T x}\\bigg) + h(y) \\\\ \u0026= \\mathop{\\arg\\min}_y \\frac{1}{2t}\\bigg( \\textcolor{red}{2t\\nabla g(x)^T(y-x)} + \\textcolor{green}{||y-x||^2} \\bigg) + h(y) \\\\ \u0026= \\mathop{\\arg\\min}_y \\nabla g(x)^T(y-x) + \\frac{L}{2}||y-x||^2 + h(y) \\end{aligned} $$ Therefore $\\phi (x; \\lambda)$ is related to the minimum objective value in the proximal operators.\nPart 1: $$ \\begin{aligned} \u0026g(x^+) \\le g(x) + \\nabla g(x)^T(x^+ - x) + \\frac{L}{2}||x^+ - x||^2 \\\\ \u0026g(x) - g(x^+) \\ge \\nabla g(x)^T(x-x^+) - \\frac{L}{2}||x^+ - x||^2 \\\\ \u0026f(x) - f(x^+) \\ge \\nabla g(x)^T(x-x^+) - \\frac{L}{2}||x^+ - x||^2 + h(x) - h(x^+) \\end{aligned} $$ We know that $$ \\begin{aligned} \u0026\\phi(x;L) = -2L (\\nabla g(x)^T(x^+ - x)+\\frac{L}{2}||x^+-x||^2 + h(x^+)) + 2Lh(x) \\\\ \u0026\\therefore \\frac{1}{2L}\\phi(x;L) = \\nabla g(x)^T(x-x^+) - \\frac{L}{2}||x^+ - x||^2 + h(x) - h(x^+) \\end{aligned} $$ Therefore: $f(x) - f(x^+) \\ge \\frac{1}{2L}\\phi (x;L)$\nPart 2: Remember $g$ is strongly convex, so: $$ \\begin{aligned} \u0026g(x^*) \\ge g(x) + \\nabla g(x)^T(x^* - x) + \\frac{m}{2}||x^* - x||_2^2 \\\\ \u0026g(x) - g(x^*) \\le \\nabla g(x)^T(x-x^*) - \\frac{m}{2}||x^*-x||_2^2 \\\\ \u0026f(x) - f(x^*) \\le \\nabla g(x)^T(x-x^*) - \\frac{m}{2}||x^*-x||_2^2 + h(x) - h(x^*) \\end{aligned} $$ $x^*$ is the optimal point, so it is not necessary to minimize the objective value in the proximal operators: $$ \\frac{1}{-2m} \\phi (x;m) \\le \\nabla g(x)^T(x^*-x) + \\frac{m}{2}||x^* - x||^2 + h(x^*) - h(x) $$ $$ \\begin{aligned} \u0026\\frac{1}{2m} \\phi (x;m) \\ge \\nabla g(x)^T(x-x^*) - \\frac{m}{2}||x^* - x||^2 + h(x) - h(x^*) \\\\ \u0026\\therefore f(x) - f(x^*) \\le \\frac{1}{2m}\\phi (x;m) \\end{aligned} $$ Because $g$ is strongly convex, so: $$ mI \\preceq \\nabla^2 g(x) \\preceq LI $$ Thus $m \\le L$, and finally: $$ \\begin{aligned} \u0026f(x) - f(x^+) \\ge \\frac{m}{L} \\bigg( f(x) - f(x^*) \\bigg) \\\\ \u0026f(x^+) - f(x^*) \\le \\bigg( 1-\\frac{m}{L} \\bigg)(f(x) - f(x^*)) \\end{aligned} $$","permalink":"https://zli1024.github.io/posts/cmu10_725_hw2/","summary":"\u003ch2 id=\"homework-2-cvx-opt-10-725\"\u003eHomework 2 CVX-OPT 10-725\u003c/h2\u003e\n\u003ch3 id=\"1-subgradients-and-proximal-operators\"\u003e1 Subgradients and Proximal Operators\u003c/h3\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://zli1024-pictures.oss-cn-shenzhen.aliyuncs.com/imgs/202506141754062.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e(i)\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eLet $g_1, g_2 \\in \\partial f(x)$ , therefore we have:\n\u003c/p\u003e\n$$\n\\begin{aligned}\nf(y) \u0026\\ge f(x) + g_1^\\top (y-x) \\\\\nf(y) \u0026\\ge f(x) + g_2^\\top (y-x) \\\\\ntf(y) \u0026\\ge tf(x) + tg_1^\\top (y-x) \\qquad \\qquad \\qquad(1)\\\\\n(1-t)f(y) \u0026\\ge (1-t)f(x) + (1-t)g_2^\\top (y-x) \\quad (2)\\\\\n\\end{aligned}\n$$\u003cp\u003e\n(1)+(2):\n\u003c/p\u003e\n$$\nf(y) \\ge f(x) + [tg_1+(1-t)g_2]^\\top(y-x)\n$$\u003cp\u003e\nThus, $g=tg_1+(1-t)g_2 \\in \\partial f$ and the subdifferential $\\partial f(x)$ is a convex set.\u003c/p\u003e","title":"CMU 10-725 HW2"},{"content":"相机噪声标定流程 第一部分：EMVA1288 相机的线性模型及噪声 像素曝光与相机的线性信号模型 首先，我们需要了解噪声生成的整个过程，并尝试建立一个物理噪声模型，以便进行噪声标定。下图（来自EMVA1288）给我们展示了一个相机的通用物理模型。\n所谓的EMVA1288标准，是欧洲机器视觉委员会专门编写的关于数字图像传感器以及相机特性的量化评估标准。该标准历史悠久，我们现在采用的是最新的4.0版本，4.0版本分为了线性版和非线性版，其中线性版本就是针对我们日常使用的数码相机、手机、单反这样常规的相机数字图像传感器，而非线性版本则是指那些不符合线性曝光过程的相机。所以本文所指的EMVA1288主要是针对线性版来展开讲解。\n从EMVA1288中我们知道，在整个相机电子系统中，由光照累积的电荷单位被转换为电压，经过放大，最终通过模数转换器（ADC）转换为数字信号$y$。而这整个过程可以看作是线性的，并且可以用具体的量来描述，如系统增益$K$（单位为$DN/e^{-}$，这里$DN$为Digital Number的缩写）等等。所以关于上图中的数字量$\\mu_y$（平均像素值）可以建立如下公式： $$ \\mu_y = K(\\mu_e + \\mu_d) \\quad or \\quad \\mu_y = \\mu_{y \\cdot dark} + K\\mu_e \\qquad (1) $$ 而平均光子数$\\mu_p=\\frac{AEt_{exp}}{h\\nu}=\\frac{AEt_{exp}}{hc/\\lambda}$，其中$A$为传感器面积，$E$是传感器表面在曝光时间$t_{exp}$ 内的光照度，单位为$W/m^2$，而平均电子数$\\mu_e=\\eta \\mu_p$​，因此上述方程(1)可以转换为以下方程： $$ \\mu_y = \\mu_{y\\cdot dark} + K\\eta \\mu_p = \\mu_{y\\cdot dark} + K\\eta\\frac{\\lambda A}{hc}Et_{exp} \\qquad (2) $$噪声模型 散粒噪声（Shot noise）是泊松分布的，因此有$\\sigma_{e}^2=\\mu_e$；根据上图噪声生成模型所示，所有与传感器读出和放大电路相关的噪声源都可以用一个方差为 $\\sigma_d^2$ 的与信号无关的正态分布噪声源来描述。最终的模数转换会在量化区间之间添加另一个均匀分布的噪声源，其方差为 $\\sigma_q^2=1/12DN^2$。由于所有噪声源的方差线性相加，根据误差的传播规则，数字信号$y$ 的总时域方差(temporal variance)$\\sigma_y^2$ 可以表示为： $$ \\sigma_y^2 = K^2(\\sigma_d^2 + \\sigma_e^2) + \\sigma_q^2 \\qquad (3) $$ 噪声可以与测量的平均数字信号相关联（利用公式(1)以及$\\sigma_{e}^2=\\mu_e$​）： $$ \\begin{matrix} \\sigma_y^2 = \\underbrace{K^2\\sigma_d^2 + \\sigma_q^2} \\qquad \\\\offset \\end{matrix} \\begin{matrix}+\\underbrace{K}(\\mu_y-\\mu_{y\\cdot dark})\\\\slope \\qquad \\quad \\qquad\\end{matrix} \\qquad(4) $$ 这个方程是传感器特性表征的核心，根据噪声方差$\\sigma_y^2$ 与光诱导数字信号均值$\\mu_y - \\mu_{y\\cdot dark}$ 之间的线性关系，可以由斜率确定整体系统增益$K$，并从偏移量确定暗噪声方差$\\sigma_d^2$ ，这种方法被称为光子转移方法（Photon Transfer Method）。\n数字信号均值和方差的计算 通过光子转移曲线（公式(4)）我们知道，要想求出整体系统增益（也就是斜率），我们需要得到一组关于数字信号的方差$\\sigma_y ^2$以及均值$\\mu_y-\\mu_{y\\cdot dark}$，以下讲解如何求出这些量：\n一、像素的均值\n首先我们需要计算像素的均值，在实际操作时我们可以这样：在每一个特定的曝光时间下，用相同的相机设置拍摄正对着相机的一个均匀图卡两次，如下图所示，而总共需要在多种曝光时间下进行：\n接下来，我们就可以求特定曝光时间下，这两幅图像中图卡所在区域的像素均值，首先在每幅图像上求均值，再把两次的均值做平均——注意这里的结果也包含了暗信号导致的像素值。如果我们调整相机拍摄的距离，可以使得整个图卡充满相机的视场，这样如果一幅图像的长宽为$M\\times N$ ，我们就可以充分利用到所有的像素来计算图像均值了。 $$ 在某个曝光时间t_{exp}下求均匀图卡的像素均值 \\\\ \\mu_y[k]=\\frac{1}{NM}\\sum_{m=0}^{M-1}\\sum_{n=0}^{N-1}y[k][m][n],(k=0,1) \\quad and \\quad \\mu_y = \\frac{1}{2}(\\mu_y[0]+\\mu_y[1]) \\qquad(5) $$ 注意这里是在一个特定曝光时间下进行的，我们可以在多个曝光时间下重复上述步骤，这样可以得到多个图像（有些文献称为flat-field frame）均值。\n现在我们遮住相机的镜头，此时没有光线进入相机。我们重复上述步骤就可以测出在多个曝光时间下的暗信号（有些文献也称为bias frame）的均值： $$ \\mu_{y\\cdot dark}[k]=\\frac{1}{NM}\\sum_{m=0}^{M-1}\\sum_{n=0}^{N-1}y_{dark}[k][m][n],(k=0,1) \\quad and \\quad \\mu_{y\\cdot dark} = \\frac{1}{2}(\\mu_{y\\cdot dark}[0]+\\mu_{y\\cdot dark}[1]) \\quad(5d) $$ 通过上面两步，我们得到了公式(4)中的$\\mu_y$ 和$\\mu_{y\\cdot dark}$\n二、信号时域噪声的方差\n当我们要求一个随机变量的均值和方差时，通常需要在时域上得到很多个这个变量的值才行。我们在前面之所以在空域上求像素的均值，是基于这样的假设：图像传感器上的每个像素之间的分布是相同的，所以我们用空域上的多个像素值代替了时域上变化的像素值（时域上我们对同一个曝光时间只拍了两幅图像，理论上同一个像素只有两个样本）。为了求得像素值在时域上的变化方差，我们可以基于同样的思想来做。那么似乎按照下面的公式来求就可以完成： $$ \\sigma_y^2=|y-\\mu_y|^2 \\approx \\frac{1}{NM} \\sum_{m=0}^{M-1}\\sum_{n=0}^{N-1}(y[m][n]-\\mu_y)^2 $$ 不过，除了现在描述的这种像素之间的一致性的随机分布，一般在传感器上还会有一种固定的空间噪声，它体现了传感器像素阵列的空间非均匀性。比如在https://homes.psd.uchicago.edu/~ejmartin/pix/20d/tests/noise/index.html#patternnoise中展示的Canon 20D的传感器的空间非均匀性，我们肉眼很容易看出这里的横向条纹，这就是这种非均匀性。所以我们在计算像素值的时域方差时，要特别小心这一点，在这种情况下，整个图像的方差由两部分构成，一个是像素值自身的波动，一个是像素阵列的空间非均匀性。 $$ \\frac{1}{NM}\\sum_{m=0}^{M-1}\\sum_{n=0}^{N-1}(y[k][m][n]-\\mu[0])^2=\\sigma_y^2 + s_y^2 \\qquad(6) $$ 由于空间非均匀性对一个传感器是固定的，所以当我们拍摄两幅图像后，就可以消去这个变量。我们用这两幅图像的方差的差，来估计单个像素时域方差，所以我们可以得到下面的公式： $$ 在某个曝光时间t_{exp}下求均匀图卡的像素方差 \\\\ \\sigma_y^2 = \\frac{1}{2NM}\\sum_{m=0}^{M-1}\\sum_{n=0}^{N-1}[(y[0][m][n]-\\mu[0])-(y[1][m][n]-\\mu[1])]^2 \\\\ = \\frac{1}{2NM}\\sum_{m=0}^{M-1}\\sum_{n=0}^{N-1}(y[0][m][n]-y[1][m][n])^2 - \\frac{1}{2}(\\mu[0]-\\mu[1])^2 \\qquad (7) $$ 当我们多次改变曝光时间时，我们将得到多个$\\sigma_y^2$ 和$\\mu_y - \\mu_{y\\cdot dark}$ ，因而由公式（4）就容易得到此时的系统增益$K$，以及总的加性噪声了$K^2\\sigma_d^2 + \\sigma_q^2$ 。\n信噪比 关于信噪比的计算公式有很多，而在EMVA1288中信噪比的计算公式如下： $$ SNR=\\frac{E(I)}{\\sigma(I)} = \\frac{\\mu_y-\\mu_{y\\cdot dark}}{\\sigma_y} \\qquad (8) $$ 由于我们已经得到一组像素均值和方差，后续将利用这一公式求出SNR的变化曲线图。\n利用公式（2）和（4），上述公式变形为： $$ SNR(\\mu_p)=\\frac{K\\eta \\mu_p}{\\sqrt{K^2\\sigma_d^2+\\sigma_q^2+K^2\\eta\\mu_p}}=\\frac{\\eta \\mu_p}{\\sqrt{\\sigma_d^2+\\sigma_q^2/K^2+\\eta\\mu_p}} $$ 可以看出，信噪比是以平均光子数为自变量的，同时这里的量化噪声一般来说相对较小，这意味着在计算信噪比的过程中，系统增益$K$​几乎可以忽略不记。那么信噪比可以说只取决于传感器的光量子效率(QE，注意它与波长相关)，以及暗噪声。\n关于空域非一致性，个人认为现今的摄像机传感器制造工艺不断在进步，针对拍摄的暗场图像没有很明显的行间非一致性，列间非一致性或者像素间非一致性，所以这里没有列出其测量方法。具体方法可以参考EMVA1288空域非一致性部分。\n第二部分：测量与评估 我们可以通过光子转移方法得到相机系统增益$K$，在此之前我们需要拍摄一组明场图像(flat-field frame)和暗场图像(bias frame)，以下是我的拍摄参数：\nSetup ISO f/# f Temperature Canon EOS M50 3200 f/5.6 15mm room temperature 设定的不同曝光时间为： 1/4000s, 1/3200s, 1/2500s, 1/2000s, 1/1600s, 1/1250s, 1/1000s, 1/800s, 1/640s, 1/500s, 1/400s, 1/320s, 1/250s, 1/200s, 1/160s, 1/125s, 1/100s, 1/80s, 1/60s, 1/50s, 1/40s, 1/30s, 1/25s, 1/20s, 1/15s, 1/13s, 1/10s, 1/8s, 1/6s, 1/5s, 1/4s, 1/3s, 0.4s, 0.5s, 0.6s, 0.8s\n总结来说，明场图像有36组（每组为在不同曝光时间，其他设置相同下连续拍摄的两张图片），暗场图像也36组（每组为在不同曝光时间，其他设置相同下连续拍摄的两张图片），所有数据都可以在目录\u0026rsquo;./images/\u0026lsquo;中找到。\n系统增益的测量 关键点：$\\sigma_y^2=offset+K(\\mu_y-\\mu_{y\\cdot dark})$\n通过光子转移方法测出的结果如下：\n该图展示了测量的方差 $\\sigma_y^2$ 与平均像素值 $\\mu_y - \\mu_{y\\cdot dark}$ 的关系，以及用于确定系统增益 $K$ 的线性回归曲线。绿色点标记了用于线性回归的0-70%的饱和范围。系统增益 $K$ 的值是通过one-sigma原则的统计不确定性（以百分比表示）得到的。\n计算代码包含在 python 文件 “noise_calibration.py” 中，对应生成上述图表的代码片段如下所示：\n拟合结果：\n斜率：$K = 7.58621139$ 截距：840.16712447\n线性范围：0~69.44%（0-70%）符合EMVA1288关于 0-70%饱和范围的标准\n信噪比的测量 关键：$SNR=\\frac{\\mu_y-\\mu_{y\\cdot dark}}{\\sigma_y}$\n计算代码包含在 python 文件 “noise_calibration.py” 中，对应的代码片段如下所示：\n在特定温度下暗电流的评估 代码实现原理在\u0026quot;Expo.py\u0026quot; 和 \u0026ldquo;ISO.py\u0026quot;这两个文件中，实现的结果如下：\n第三部分：ELD噪声模型 原论文：Physics-Based Noise Modeling for Extreme Low-Light Photography\n该论文提出了一个与EMVA1288稍微有点区别的物理噪声模型，具体分析来说：\n1）将读出噪声分为了暗电流噪声$N_d$ 、源随器噪声$N_s$ 以及热噪声$N_t$ ；\n而EMVA1288在读出噪声上仅考虑的是暗信号的噪声：其中暗信号对应的是暗电荷，它并不是一个固定的值，而是一个与曝光时间和温度都相关的电荷值。具体来说它由两部分组成，其单位是电荷/像素，即每像素平均的暗电荷值。 $$ 暗电荷由两部分组成 \\\\ \\mu_d=\\mu_{d\\cdot 0} +\\mu_{therm}=\\mu_{d\\cdot 0} + \\mu_{I\\cdot y}t_{exp} \\qquad (9) $$ 这里第一部分是一个与曝光时间无关的部分，主要是各种电子电路引起的噪声，而$\\mu_{d\\cdot0}$是它的平均值。\n而第二部分是一个与曝光时间直接相关的量，同时也是与温度相关，这一部分可以被称为热电荷，其中$\\mu_{I\\cdot y}$是所谓的热电流，它的单位是$e^-/(pixel \\cdot s)$ ，即电荷/像素秒。\n2）空域非一致性主要考虑的是行间非一致性；\n3）**将读出噪声和行噪声放到系统增益之后；**关于这一点我有点疑惑，因为读出噪声和行噪声是在电子转换为电压这一阶段的，这在原论文也有提到，但是在公式中却没有体现出读出噪声和行噪声被系统增益$K$ 放大的影响。其实更准确地来说，根据High-level numerical simulations of noise in CCD and CMOS photosensors: review and tutorial论文中的说法，$N_d$和$N_t$ 是在增益放大前（这两噪声是在Electrons阶段），而$N_s$ 是在增益放大后（在Voltage阶段），所以应该分开来讨论。不过原论文作者可能为了简化分析，就可能没分开来讨论各自的影响了。\n总而言之，ELD的总体噪声模型可以用以下公式来表示： $$ N=KN_p+N_{read}+N_r+N_q \\qquad(10) $$a)光子散粒噪声$N_p$​ 光子散粒噪声$N_p$服从的分布：$(I+N_p) \\sim \\mathcal{P}(I)$\n当得出$K$之后，就可以将原始数字信号$D$转换为光电子数$I$ : 可通过$\\mu_y - \\mu_{y\\cdot dark} = K \\mu_e$计算出$\\mu_e$；\n这里将原论文中的公式和EMVA1288作对比以直观地展示各变量的含义：\nELD：$Var(D) = K^2I+Var(N_o)=K(KI)+Var(N_o)$\nEMVA1288：$\\sigma_y^2 = K^2\\sigma_d^2+\\sigma_q^2 + K(\\mu_y - \\mu_{y\\cdot dark})$​\n所以这里可以很清楚地看出$KI=\\mu_y-\\mu_{y\\cdot dark}$\n然后对其施加泊松分布：因为光电子数服从泊松分布，其各个曝光时间的均值已知，所以可以计算出其PMF；\n具体来说，因为在每一特定曝光时间下的均值$I=\\frac{\\mu_y-\\mu_{y\\cdot dark}}{K}$ 可以确定，也就是泊松分布的$\\lambda$ 也已确定，所以可以轻易计算出PMF，下图为四个特定曝光时间下的例子：\n最后将其还原回$D$，进而模拟了真实的光子散粒噪声：其实就是直接利用了$K \\mu_e = \\mu_y - \\mu_{y\\cdot dark}$ ，这样就表示了在未加噪声前的数字信号$D'=K\\mu_e$ 而$D = D' + N$，这里的$N = KN_p+N_{read} + N_r + N_q$\n具体实现代码：\n在文件noise_calibration.py中，实现代码片段如下：\nb)颜色偏差$\\mu_c$ 由于相机成像过程（或者说CMOS传感器中黑电平的存在），全黑状态下拍摄的暗场图中像素值的平均值并不为零，而是应该处于传感器黑电平的位置，而ELD论文提出传感器中直流电噪声会导致一部分的颜色偏差，也就是说直流电噪声会使得像素值均值在黑电平上下波动，因此做了如下测量和评估：\n对于给定一张暗场图像，可以求各个颜色通道的平均值来算出每个通道与其黑电平的偏差。下图所示为在36张暗场图像中统计出的颜色偏差（所用设备为佳能M50）：\n与原论文进行对比：\n可以发现佳能M50的偏差不大，所以应该可以推测出随着现如今制造工艺的进步，这方面的误差应该也会越来越小了。\n关于如何得到黑电平：可以通过dcraw来获取\ndcraw是一个非常有名的软件，专门用于解析RAW格式的图像，在它的官网上列出了大量利用了dcraw进行核心解析代码的软件，可以点击去查看下。在Ubuntu上，只需要在终端输入sudo apt-get install dcraw就可以安装该软件了。（不过需要注意的是dcraw不支持.CR3格式，对此我用的是exiftool工具包）\n示例：\nexport input_file=\u0026rdquo;./images/_MG_0771.CR2\u0026quot;\ndcraw -4 -d -v -T $input_file\n通过以上两个命令就可以得到关于黑电平和其他信息\n佳能M50各通道黑电平：2048 2048 2048 2048\n具体实现代码：\n在文件color_bias.py中，实现代码片段如下：\nc)行噪声$N_{r}$ 原论文说是直接对bias frames进行离散傅里叶变换，这里我对曝光时间最小（Expo = 1/4000s）的bias frame进行DFT，用的np.fft，同时对明显的行噪声和列噪声（来源：网上找的例子）进行对比\nnp.fft.fftshift的作用：通过将零频分量移动到数组中心，重新排列傅里叶变换的结果\n我们知道：经过FFT之后，输出的频率范围是[0,fs]，但是，我们研究的范围一般是[-fs/2, fs/2]，也就是零频在中间，因此就需要将FFT的结果通过fftshift处理一下，将零频分量移到序列中间。\nBias frame(1/4000s):\n这么一对比，Canon M50拍出来的bias frames行噪声就不是很明显。\n具体实现代码：\n在文件row_noise.py中，实现代码片段如下：\n在接下去之前，论文第六页在Estimate $\\mu_c$​ for color bias 结尾处提到为了消除直流噪声对后续其他噪声参数估计的影响，先从暗场图像中减去每个颜色通道的平均值，这里我每个通道减去的值都取为2048\n行噪声服从均值为零的高斯分布：$N_r \\sim \\mathcal{N}(o, \\sigma_r)$\n关于参数$\\sigma_r$ 是如何估计的：首先求出暗场图像每行的均值（这里论文的意思好像不需要减去黑电平也能求出$\\sigma_r$），然后相应地就可以得到每一行的方差，最后行噪声就是由这么一组按行排列的高斯分布采样得到的信号值。关于这部分的实现原理去read_noise.py看代码实现会更清楚直观一些。\n######### Row noise parameters ######### # extract mean values from each row mu_row = np.sum(raw_data, axis=1) / w mu_row2d = mu_row.reshape(h, 1) mu_row_t = np.tile(mu_row2d, w) # maximizing the log-likelihood sigma2 = np.sum((raw_data - mu_row_t)**2, axis=1) / w ######################## Step 2: row noise image ###################### raw_row = np.zeros((h, w), dtype=np.float64) for i in range(h): raw_row[i][:] = np.random.normal(loc=mu_row[i], scale=np.sqrt(sigma2[i]), size=w) c) 读噪声$N_{read}$ 补充下关于Probability Plot的知识：\nThe probability plot is a way of visually comparing the data coming from different distributions. These data can be of empirical dataset or theoretical dataset. The probability plot can be of two types:\nP-P plot: The (Probability-to-Probability) p-p plot is the way to visualize the comparing of cumulative distribution function (CDFs) of the two distributions (empirical and theoretical) against each other. Q-Q plot: The q-q (Quantile-to-Quantile) plot is used to compare the quantiles of two distributions. The quantiles can be defined as continuous intervals with equal probabilities or dividing the samples between a similar way The distributions may be theoretical or sample distributions from a process, etc. The normal probability plot is a case of the q-q plot. Normal Probability plot: The normal probability plot is a way of knowing whether the dataset is normally distributed or not. In this plot, data is plotted against the theoretical normal distribution plot in a way such that if a given dataset is normally distributed it should form an approximate straight line. The normal probability plot is a case of the probability plot (more specifically Q-Q plot). This plot is commonly used in the industry for finding the deviation from the normal process.\nSource: https://www.geeksforgeeks.org/normal-probability-plot/\n1. Gaussian Probability Plot\nPreprocess:\n用暗场图像先减去上一步的行噪声，然后为了加速运算，我在代码中取图像正中间一小块部分来运算（不求速度的话可以用整张图的数据去算分布），然后就可以画Q-Q图了。\n结果发现和正态分布拟合地很好，接下来看看和Tukey lambda分布拟合情况如何。\n2. Tukey Lambda PPCC Plot\n补充知识：\n*Reference: *\n1.https://www.itl.nist.gov/div898/software/dataplot/refman1/auxillar/ppccplot.htm\n2.https://moonapi.com/news/4061.html\n3.https://www.itl.nist.gov/div898/handbook/eda/section3/eda366f.htm\nThe Tukey Lambda PPCC plot, with shape parameter λ, is particularly useful for symmetric distributions. It indicates whether a distribution is short or long tailed and it can further indicate several common distributions. Specifically:\nλ = -1: distribution is approximately Cauchy λ = 0: distribution is exactly logistic λ = 0.14: distribution is approximately normal λ = 0.5: distribution is U-shaped λ = 1: distribution is exactly uniform If the Tukey Lambda PPCC plot gives a maximum value near 0.14, we can reasonably conclude that the normal distribution is a good model for the data. If the maximum value is less than 0.14, a long-tailed distribution such as the double exponential or logistic would be a better choice. If the maximum value is near -1, this implies the selection of very long-tailed distribution, such as the Cauchy. If the maximum value is greater than 0.14, this implies a short-tailed distribution such as the Beta or uniform.\nSource: https://www.itl.nist.gov/div898/software/dataplot/refman1/auxillar/ppccplot.htm\nPPCC图可以用来求分布最符合的形状参数$\\lambda$ ，结果如上：$\\lambda = 0.164$ 表明了在Canon M50拍下的暗场图像呈短尾分布（$-1 \u003c\\lambda \u003c 0.14$为长尾分布；$\\lambda=0.14$呈完美的正态分布；$\\lambda\u003e0.14$​ 呈短尾分布）\n注意在用Tukey lambda分布估计数据分布时要事先画直方图看数据是否对称分布\nAs the Tukey-Lambda distribution is a symmetric distribution, the use of the Tukey-Lambda PPCC plot to determine a reasonable distribution to model the data only applies to symmetric distributions. A histogram of the data should provide evidence as to whether the data can be reasonably modeled with a symmetric distribution.\n3. Tukey Lambda Probability Plot\n由于在PPCC图中我们得到了关于暗场图像的分布形状参数，也就是shape_param_max，即shape_param_max = $\\lambda = 0.164$ ，所以也就得到了形状参数为0.164的Tukey lambda分布来估计暗场图像的分布，标定完后续就可以用这个分布来建立数据集。\n用形状参数$\\lambda = 0.164$ 的分布来拟合发现和正态分布的Probability Plot拟合程度$R^2$区别不大，所以读出噪声可以近似为正态分布；关于Tukey Lambda分布的拟合情况请看下图（这里Tukey Lambda分布的参数$\\lambda$ 我取的是-0.14，即原论文中佳能EOS70D的参数来进行对比）：\n可以发现佳能M50的读出噪声对Tukey Lambda的分布拟合情况并不好，不如正态分布；不过发现在噪声极少的情况下（曝光时间30s，ISO为200），读出噪声更符合Tukey Lambda的分布，原因可能是当噪声变多后分布符合大数定律的原则。\n具体实现代码：\n在文件read_noise.py中，实现代码片段如下：\nd)重建流程 这一步是为了模拟仿真出真实相机传感器中产生的各种噪声，也就是利用公式（10）复原上述所讲的各种噪声，然后进行ISP流程如：线性化\u0026ndash;\u0026gt;白平衡\u0026ndash;\u0026gt;去马赛克\u0026ndash;\u0026gt;颜色矫正\u0026ndash;\u0026gt;亮度拉伸/Gamma矫正，最后重建出一张噪声图。\n首先通过直方图直观地看重建的噪声图和原暗场图像之间的差异：\n图形差异挺大的，原因可能是在原暗场图像中存在不少离群点，那我们用数据差异来看二者之间差异多大；这里用的指标是$R^2$ 和KL散度，其中$R^2$ 作了一些处理（由于噪声是随机出现在图像不同像素点位置的，所以为了避免位置对数值上差异的干扰，代码中对两幅图像素值作了排序，这样算$R^2$ 才不会出现负值），结果如下：\nR2 for estimated model: 0.9972249549587314\nKL-divergence: 0.0001303227610267369\n可以看出最后重建的效果不错，噪声图比较接近真实的暗场图像。\n关于为何在代码中为何不加入光子散粒噪声：\n因为这里重建的是暗场图像，不受实际光子影响，所以没加。如果想在一幅真实环境下图像加噪声，此时就可以考虑光子散粒噪声$N_p$的影响了\n最后重建出来的效果如下（色调有点偏白，不过不是重建中出现的问题）：\n**具体实现代码：**在文件noise_img.py中可以直观地看每一步实现的细节\n","permalink":"https://zli1024.github.io/posts/%E7%9B%B8%E6%9C%BA%E5%99%AA%E5%A3%B0%E6%A0%87%E5%AE%9A/","summary":"\u003ch1 id=\"相机噪声标定流程\"\u003e相机噪声标定流程\u003c/h1\u003e\n\u003ch2 id=\"第一部分emva1288-相机的线性模型及噪声\"\u003e第一部分：EMVA1288 相机的线性模型及噪声\u003c/h2\u003e\n\u003ch3 id=\"像素曝光与相机的线性信号模型\"\u003e像素曝光与相机的线性信号模型\u003c/h3\u003e\n\u003cp\u003e首先，我们需要了解噪声生成的整个过程，并尝试建立一个物理噪声模型，以便进行噪声标定。下图（来自EMVA1288）给我们展示了一个相机的通用物理模型。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://zli1024-pictures.oss-cn-shenzhen.aliyuncs.com/imgs/202506112302533.png\"\u003e\u003c/p\u003e\n\u003cp\u003e所谓的EMVA1288标准，是欧洲机器视觉委员会专门编写的关于数字图像传感器以及相机特性的量化评估标准。该标准历史悠久，我们现在采用的是最新的4.0版本，4.0版本分为了线性版和非线性版，其中线性版本就是针对我们日常使用的数码相机、手机、单反这样常规的相机数字图像传感器，而非线性版本则是指那些不符合线性曝光过程的相机。所以本文所指的EMVA1288主要是针对线性版来展开讲解。\u003c/p\u003e\n\u003cp\u003e从EMVA1288中我们知道，在整个相机电子系统中，由光照累积的电荷单位被转换为电压，经过放大，最终通过模数转换器（ADC）转换为数字信号$y$。而这整个过程可以看作是线性的，并且可以用具体的量来描述，如系统增益$K$（单位为$DN/e^{-}$，这里$DN$为Digital Number的缩写）等等。所以关于上图中的数字量$\\mu_y$（平均像素值）可以建立如下公式：\n\u003c/p\u003e\n$$\n\\mu_y = K(\\mu_e + \\mu_d) \\quad or \\quad \\mu_y = \\mu_{y \\cdot dark} + K\\mu_e  \\qquad (1)\n$$\u003cp\u003e\n而平均光子数$\\mu_p=\\frac{AEt_{exp}}{h\\nu}=\\frac{AEt_{exp}}{hc/\\lambda}$，其中$A$为传感器面积，$E$是传感器表面在曝光时间$t_{exp}$ 内的光照度，单位为$W/m^2$，而平均电子数$\\mu_e=\\eta \\mu_p$​，因此上述方程(1)可以转换为以下方程：\n\u003c/p\u003e\n$$\n\\mu_y = \\mu_{y\\cdot dark} + K\\eta \\mu_p = \\mu_{y\\cdot dark} + K\\eta\\frac{\\lambda A}{hc}Et_{exp}  \\qquad (2)\n$$\u003ch3 id=\"噪声模型\"\u003e噪声模型\u003c/h3\u003e\n\u003cp\u003e散粒噪声（Shot noise）是\u003cstrong\u003e泊松分布\u003c/strong\u003e的，因此有$\\sigma_{e}^2=\\mu_e$；根据上图噪声生成模型所示，所有与传感器读出和放大电路相关的噪声源都可以用一个方差为 $\\sigma_d^2$ 的与信号无关的\u003cstrong\u003e正态分布\u003c/strong\u003e噪声源来描述。最终的模数转换会在量化区间之间添加另一个\u003cstrong\u003e均匀分布\u003c/strong\u003e的噪声源，其方差为 $\\sigma_q^2=1/12DN^2$。由于所有噪声源的方差线性相加，根据误差的传播规则，数字信号$y$ 的总时域方差(temporal variance)$\\sigma_y^2$ 可以表示为：\n\u003c/p\u003e\n$$\n\\sigma_y^2 = K^2(\\sigma_d^2 + \\sigma_e^2) + \\sigma_q^2 \\qquad (3)\n$$\u003cp\u003e\n噪声可以与测量的平均数字信号相关联（利用公式(1)以及$\\sigma_{e}^2=\\mu_e$​）：\n\u003c/p\u003e\n$$\n\\begin{matrix} \\sigma_y^2 = \\underbrace{K^2\\sigma_d^2 + \\sigma_q^2} \\qquad \\\\offset    \\end{matrix}\n\\begin{matrix}+\\underbrace{K}(\\mu_y-\\mu_{y\\cdot dark})\\\\slope \\qquad \\quad \\qquad\\end{matrix}  \\qquad(4)\n$$\u003cp\u003e\n\u003cstrong\u003e这个方程是传感器特性表征的核心，根据噪声方差$\\sigma_y^2$ 与光诱导数字信号均值$\\mu_y - \\mu_{y\\cdot dark}$ 之间的线性关系，可以由斜率确定整体系统增益$K$，并从偏移量确定暗噪声方差$\\sigma_d^2$ ，这种方法被称为光子转移方法（Photon Transfer Method）。\u003c/strong\u003e\u003c/p\u003e","title":"相机噪声标定"}]